{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5005)\n",
      "/usr/local/anaconda3/envs/regression_dragonn_new/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from dragonn import models\n",
    "from dragonn.plot import add_letters_to_axis\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "from warnings import warn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_to_seq = OrderedDict()\n",
    "seq_len = 145\n",
    "reg_len = 295\n",
    "skip_len = 5\n",
    "\n",
    "with open(\"../../data/Scaleup_counts_sequences/ScaleUpDesign1.sequences.txt\") as f:\n",
    "    for line in f:\n",
    "        key, seq = line.strip().split()\n",
    "        \n",
    "        # TODO: Figure out if this is an OK thing to do. 'N' basically means the \n",
    "        # sequencing software couldn't figure out what the base was...?\n",
    "        if \"N\" in seq:\n",
    "            seq = seq.replace(\"N\", \"A\")\n",
    "        \n",
    "        assert key not in key_to_seq\n",
    "        key_to_seq[key] = seq\n",
    "        \n",
    "with open(\"../../data/Scaleup_counts_sequences/ScaleUpDesign2.sequences.txt\") as f:\n",
    "    for line in f:\n",
    "        key, seq = line.strip().split()\n",
    "        \n",
    "        if \"N\" in seq:\n",
    "            seq = seq.replace(\"N\", \"A\")\n",
    "        \n",
    "        assert key not in key_to_seq\n",
    "        key_to_seq[key] = seq\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "sample_weights = {}\n",
    "cell_types =  [\"HepG2\", \"K562\"]\n",
    "promoters = [\"SV40P\", \"minP\"]\n",
    "design_names = [\"ScaleUpDesign1\", \"ScaleUpDesign2\"]\n",
    "\n",
    "for cell_type in cell_types:\n",
    "    for promoter in promoters:\n",
    "        experiment_key = (cell_type, promoter)\n",
    "        data[experiment_key] = {}\n",
    "        sample_weights[experiment_key] = {}\n",
    "\n",
    "        for design_name in design_names:\n",
    "\n",
    "            with open(\"../../data/Scaleup_normalized/{}_{}_{}_mRNA_Rep1.normalized\".format(cell_type, design_name, promoter)) as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "\n",
    "                    key = parts[0]\n",
    "                    val = float(parts[1])\n",
    "                    if parts[2] == \"1\":\n",
    "                        data[experiment_key][key] = val\n",
    "\n",
    "            with open(\"../../data/Scaleup_normalized/{}_{}_{}_mRNA_Rep2.normalized\".format(cell_type, design_name, promoter)) as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "\n",
    "                    key = parts[0]\n",
    "                    val = float(parts[1])\n",
    "                    if parts[2] == \"1\" and key in data[experiment_key]:\n",
    "                        dot_prod = (val + data[experiment_key][key])\n",
    "                        norm = math.sqrt(2)*math.sqrt(val**2 + data[experiment_key][key]**2)\n",
    "                        cos = dot_prod/norm\n",
    "                        sample_weights[experiment_key][key] = abs(cos)\n",
    "                        data[experiment_key][key] = (val + data[experiment_key][key]) / 2.0\n",
    "    \n",
    "# One hot encode DNA sequences the standard way.\n",
    "bases = ['A', 'T', 'C', 'G']\n",
    "\n",
    "def one_hot_encode_seq(seq):\n",
    "    result = np.zeros((len(bases), len(seq)))\n",
    "    \n",
    "    for i, base in enumerate(seq):\n",
    "        result[bases.index(base), i] = 1\n",
    "\n",
    "    return result\n",
    "\n",
    "def one_hot_encode_reg(reg):\n",
    "    result = np.zeros((len(bases), reg_len))\n",
    "    \n",
    "    key = reg[0]\n",
    "    parts = key.split('_')\n",
    "    tile_pos = int(parts[3])\n",
    "    \n",
    "    seq = reg[1]\n",
    "    \n",
    "    for i, base in enumerate(seq):\n",
    "        result[bases.index(base), i + (tile_pos * skip_len)] = 1\n",
    "\n",
    "    return result\n",
    "\n",
    "def seqs_to_encoded_matrix(seqs):\n",
    "    # Wrangle the data into a shape that Dragonn wants.\n",
    "    result = np.concatenate(\n",
    "        map(one_hot_encode_seq, seqs)\n",
    "    ).reshape(\n",
    "        len(seqs), 1, len(bases), len(seqs[0])\n",
    "    )\n",
    "    \n",
    "    # Check we actually did the encoding right.\n",
    "    for i in range(len(seqs)):\n",
    "        for j in range(len(seqs[0])):\n",
    "            assert sum(result[i, 0, :, j]) == 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "def regs_to_encoded_matrix(regs):\n",
    "    # Wrangle the data into a shape that Dragonn wants.\n",
    "    result = np.concatenate(\n",
    "        map(one_hot_encode_reg, regs)\n",
    "    ).reshape(\n",
    "        len(regs), 1, len(bases), reg_len\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "valid_keys = list(reduce(\n",
    "    lambda acc, d: acc.intersection(d.keys()), \n",
    "    data.values()[1:], \n",
    "    set(data.values()[0].keys())\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/'\n",
    "rebuild = True\n",
    "\n",
    "if rebuild:\n",
    "\n",
    "    X_t = seqs_to_encoded_matrix([key_to_seq[key] for key in valid_keys])\n",
    "    X_r = regs_to_encoded_matrix([(key, key_to_seq[key]) for key in valid_keys])\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "    experiment_labels = []\n",
    "    weights = []\n",
    "    for experiment_key, key_to_normalized in data.items():\n",
    "\n",
    "        filtered_normalized = np.array([key_to_normalized[key] for key in valid_keys]).reshape(-1, 1)\n",
    "        filtered_weights = np.array([sample_weights[experiment_key][key] for key in valid_keys]).reshape(-1, 1)\n",
    "        \n",
    "        scaled = scaler.fit_transform(filtered_normalized)\n",
    "\n",
    "        experiment_labels.append(scaled)\n",
    "        weights.append(filtered_weights)\n",
    "\n",
    "    y = np.hstack(experiment_labels)\n",
    "    weights = np.hstack(weights).mean(axis=1).reshape(-1,1)\n",
    "\n",
    "    X = X_r\n",
    "\n",
    "    tasks = data.keys()\n",
    "    \n",
    "    #np.save(data_dir + 'X_t.npy', X_t)\n",
    "    #np.save(data_dir + 'X_r.npy', X_r)\n",
    "    #np.save(data_dir + 'y.npy', y)\n",
    "    np.save(data_dir + 'tasks.npy', tasks)\n",
    "\n",
    "else:\n",
    "    \n",
    "    X_t = np.load(data_dir + 'X_t.npy')\n",
    "    X_r = np.load(data_dir + 'X_r.npy')\n",
    "    y = np.load(data_dir + 'y.npy')\n",
    "    tasks = np.load(data_dir + 'tasks.npy')\n",
    "    \n",
    "X_train, X_valid, y_train, y_valid, weights_train, weights_valid = train_test_split(\n",
    "    X, y, weights, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221927, 1)\n",
      "(221927, 1, 4, 295)\n"
     ]
    }
   ],
   "source": [
    "print(weights_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "fn = \"model\"\n",
    "\n",
    "if os.path.isfile(fn + \".arch.json\") and os.path.isfile(fn + \".weights.h5\"):\n",
    "    model = models.SequenceDNN_Regression.load(fn + \".arch.json\", fn + \".weights.h5\")\n",
    "else:\n",
    "    model = models.SequenceDNN_Regression(\n",
    "        seq_length=X_train.shape[3],\n",
    "        num_filters=[100, 100],\n",
    "        conv_width=[15, 15],\n",
    "        pool_width=40,\n",
    "        num_tasks=y_train.shape[1],\n",
    "        dropout=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'train_sample_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c17e6249db7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.train(X_train, y_train, (X_valid, y_valid), \n\u001b[0;32m----> 2\u001b[0;31m             train_sample_weight=weights_train, valid_sample_weight=weights_valid)\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: train() got an unexpected keyword argument 'train_sample_weight'"
     ]
    }
   ],
   "source": [
    "model.train(X_train, y_train, (X_valid, y_valid), \n",
    "            train_sample_weight=weights_train, valid_sample_weight=weights_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.plot_architecture(fn + '.png')\n",
    "models.SequenceDNN_Regression.save(model, fn + '.arch.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "def print_perf(model, metric):\n",
    "    train_losses, valid_losses = [np.array([epoch_metrics[metric] for epoch_metrics in metrics])\n",
    "                                  for metrics in (model.train_metrics, model.valid_metrics)]\n",
    "\n",
    "    # Pretty sure early stopping works by taking the mean of losses, might want to double check\n",
    "    mean_train_losses = train_losses.mean(axis=1)\n",
    "    mean_valid_losses = valid_losses.mean(axis=1)\n",
    "    min_loss_indx = min(enumerate(mean_valid_losses), key=lambda x: x[1])[0]\n",
    "    \n",
    "    gs = gridspec.GridSpec(3, 2)\n",
    "    f = plt.figure(figsize=(15,10))\n",
    "\n",
    "    for i in range(train_losses.shape[1]):\n",
    "        y_max = max(max(train_losses[:,i]), max(valid_losses[:,i])) * 1.1\n",
    "\n",
    "        ax = f.add_subplot(gs[i])\n",
    "\n",
    "        ax.plot(range(len(train_losses[:,i])), train_losses[:,i], label='Training',lw=2)\n",
    "        ax.plot(range(len(train_losses[:,i])), valid_losses[:,i], label='Validation', lw=2)\n",
    "        \n",
    "        ax.plot([min_loss_indx, min_loss_indx], [0, y_max], 'k--', label='Early Stop')\n",
    "        if i == 0:\n",
    "            ax.legend(loc=\"best\")\n",
    "            ax.set_ylabel(metric)\n",
    "        ax.set_ylim((0,y_max))\n",
    "        ax.set_title(\"Task {}\".format(i))\n",
    "\n",
    "    y_max = max(max(mean_train_losses), max(mean_valid_losses)) * 1.1\n",
    "\n",
    "    ax = f.add_subplot(gs[train_losses.shape[1]])\n",
    "    ax.plot(range(len(mean_train_losses)), mean_train_losses, label='Training',lw=2)\n",
    "    ax.plot(range(len(mean_valid_losses)), mean_valid_losses, label='Validation', lw=2)\n",
    "        \n",
    "    ax.plot([min_loss_indx, min_loss_indx], [0, y_max], 'k--', label='Early Stop')\n",
    "    ax.set_ylim((0,y_max))\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_title(\"Mean losses\")\n",
    "        \n",
    "    plt.savefig(\"losses.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric = \"Mean Squared Error\"\n",
    "print_perf(model, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:regression_dragonn_new]",
   "language": "python",
   "name": "conda-env-regression_dragonn_new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
