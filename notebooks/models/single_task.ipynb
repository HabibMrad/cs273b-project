{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5005)\n",
      "/usr/local/anaconda3/envs/regression_dragonn_new/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from dragonn import models\n",
    "from dragonn.plot import add_letters_to_axis\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "from warnings import warn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key_to_seq = OrderedDict()\n",
    "seq_len = 145\n",
    "reg_len = 295\n",
    "skip_len = 5\n",
    "\n",
    "with open(\"../../data/Scaleup_counts_sequences/ScaleUpDesign1.sequences.txt\") as f:\n",
    "    for line in f:\n",
    "        key, seq = line.strip().split()\n",
    "        \n",
    "        # TODO: Figure out if this is an OK thing to do. 'N' basically means the \n",
    "        # sequencing software couldn't figure out what the base was...?\n",
    "        if \"N\" in seq:\n",
    "            seq = seq.replace(\"N\", \"A\")\n",
    "        \n",
    "        assert key not in key_to_seq\n",
    "        key_to_seq[key] = seq\n",
    "        \n",
    "with open(\"../../data/Scaleup_counts_sequences/ScaleUpDesign2.sequences.txt\") as f:\n",
    "    for line in f:\n",
    "        key, seq = line.strip().split()\n",
    "        \n",
    "        if \"N\" in seq:\n",
    "            seq = seq.replace(\"N\", \"A\")\n",
    "        \n",
    "        assert key not in key_to_seq\n",
    "        key_to_seq[key] = seq\n",
    "        \n",
    "data = {}\n",
    "cell_types =  [\"HepG2\", \"K562\"]\n",
    "promoters = [\"SV40P\", \"minP\"]\n",
    "design_names = [\"ScaleUpDesign1\", \"ScaleUpDesign2\"]\n",
    "\n",
    "for cell_type in cell_types:\n",
    "    for promoter in promoters:\n",
    "        experiment_key = (cell_type, promoter)\n",
    "        data[experiment_key] = {}\n",
    "\n",
    "        for design_name in design_names:\n",
    "\n",
    "            with open(\"../../data/Scaleup_normalized/{}_{}_{}_mRNA_Rep1.normalized\".format(cell_type, design_name, promoter)) as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "\n",
    "                    key = parts[0]\n",
    "                    val = float(parts[1])\n",
    "                    if parts[2] == \"1\":\n",
    "                        data[experiment_key][key] = val\n",
    "\n",
    "            with open(\"../../data/Scaleup_normalized/{}_{}_{}_mRNA_Rep2.normalized\".format(cell_type, design_name, promoter)) as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "\n",
    "                    key = parts[0]\n",
    "                    val = float(parts[1])\n",
    "                    if parts[2] == \"1\" and key in data[experiment_key]:\n",
    "                        data[experiment_key][key] = (val + data[experiment_key][key]) / 2.0\n",
    "    \n",
    "# One hot encode DNA sequences the standard way.\n",
    "bases = ['A', 'T', 'C', 'G']\n",
    "\n",
    "def one_hot_encode_seq(seq):\n",
    "    result = np.zeros((len(bases), len(seq)))\n",
    "    \n",
    "    for i, base in enumerate(seq):\n",
    "        result[bases.index(base), i] = 1\n",
    "\n",
    "    return result\n",
    "\n",
    "def one_hot_encode_reg(reg):\n",
    "    result = np.zeros((len(bases), reg_len))\n",
    "    \n",
    "    key = reg[0]\n",
    "    parts = key.split('_')\n",
    "    tile_pos = int(parts[3])\n",
    "    \n",
    "    seq = reg[1]\n",
    "    \n",
    "    for i, base in enumerate(seq):\n",
    "        result[bases.index(base), i + (tile_pos * skip_len)] = 1\n",
    "\n",
    "    return result\n",
    "\n",
    "def seqs_to_encoded_matrix(seqs):\n",
    "    # Wrangle the data into a shape that Dragonn wants.\n",
    "    result = np.concatenate(\n",
    "        map(one_hot_encode_seq, seqs)\n",
    "    ).reshape(\n",
    "        len(seqs), 1, len(bases), len(seqs[0])\n",
    "    )\n",
    "    \n",
    "    # Check we actually did the encoding right.\n",
    "    for i in range(len(seqs)):\n",
    "        for j in range(len(seqs[0])):\n",
    "            assert sum(result[i, 0, :, j]) == 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "def regs_to_encoded_matrix(regs):\n",
    "    # Wrangle the data into a shape that Dragonn wants.\n",
    "    result = np.concatenate(\n",
    "        map(one_hot_encode_reg, regs)\n",
    "    ).reshape(\n",
    "        len(regs), 1, len(bases), reg_len\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "valid_keys = list(reduce(\n",
    "    lambda acc, d: acc.intersection(d.keys()), \n",
    "    data.values()[1:], \n",
    "    set(data.values()[0].keys())\n",
    "))\n",
    "\n",
    "data_dir = 'data/'\n",
    "rebuild = True\n",
    "\n",
    "if rebuild:\n",
    "\n",
    "    X_t = seqs_to_encoded_matrix([key_to_seq[key] for key in valid_keys])\n",
    "    X_r = regs_to_encoded_matrix([(key, key_to_seq[key]) for key in valid_keys])\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "    experiment_labels = []\n",
    "    for experiment_key, key_to_normalized in data.items():\n",
    "\n",
    "        filtered_normalized = np.array([key_to_normalized[key] for key in valid_keys]).reshape(-1, 1)\n",
    "        scaled = scaler.fit_transform(filtered_normalized)\n",
    "\n",
    "        experiment_labels.append(scaled)\n",
    "\n",
    "    y = np.hstack(experiment_labels)\n",
    "\n",
    "    X = X_r\n",
    "\n",
    "    tasks = data.keys()\n",
    "    \n",
    "    #np.save(data_dir + 'X_t.npy', X_t)\n",
    "    #np.save(data_dir + 'X_r.npy', X_r)\n",
    "    #np.save(data_dir + 'y.npy', y)\n",
    "    np.save(data_dir + 'tasks.npy', tasks)\n",
    "\n",
    "else:\n",
    "    \n",
    "    X_t = np.load(data_dir + 'X_t.npy')\n",
    "    X_r = np.load(data_dir + 'X_r.npy')\n",
    "    y = np.load(data_dir + 'y.npy')\n",
    "    tasks = np.load(data_dir + 'tasks.npy')\n",
    "    \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "dnns = []\n",
    "for task in tasks:\n",
    "    dnns.append(\n",
    "        models.SequenceDNN_Regression\n",
    "        (\n",
    "            seq_length=X_train.shape[3],\n",
    "            num_filters=[100, 100],\n",
    "            conv_width=[15, 15],\n",
    "            pool_width=40,\n",
    "            num_tasks=1,\n",
    "            dropout = 0.1\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model (* indicates new best result)...\n",
      "Epoch 1:\n",
      "Train Mean Squared Error: 0.0304\tMean Absolute Error: 0.1356\t Median Absolute Error: 0.1090\t R2 Score: 0.0485\n",
      "Valid Mean Squared Error: 0.0316\tMean Absolute Error: 0.1379\t Median Absolute Error: 0.1103\t R2 Score: 0.0083 *\n",
      "Epoch 2:\n",
      "Train Mean Squared Error: 0.0267\tMean Absolute Error: 0.1272\t Median Absolute Error: 0.1017\t R2 Score: 0.1619\n",
      "Valid Mean Squared Error: 0.0287\tMean Absolute Error: 0.1314\t Median Absolute Error: 0.1045\t R2 Score: 0.0999 *\n",
      "Epoch 3:\n",
      "Train Mean Squared Error: 0.0240\tMean Absolute Error: 0.1218\t Median Absolute Error: 0.1000\t R2 Score: 0.2469\n",
      "Valid Mean Squared Error: 0.0263\tMean Absolute Error: 0.1272\t Median Absolute Error: 0.1043\t R2 Score: 0.1742 *\n",
      "Epoch 4:\n",
      "Train Mean Squared Error: 0.0229\tMean Absolute Error: 0.1198\t Median Absolute Error: 0.0997\t R2 Score: 0.2818\n",
      "Valid Mean Squared Error: 0.0254\tMean Absolute Error: 0.1259\t Median Absolute Error: 0.1045\t R2 Score: 0.2021 *\n",
      "Epoch 5:\n",
      "Train Mean Squared Error: 0.0238\tMean Absolute Error: 0.1197\t Median Absolute Error: 0.0952\t R2 Score: 0.2527\n",
      "Valid Mean Squared Error: 0.0263\tMean Absolute Error: 0.1257\t Median Absolute Error: 0.1000\t R2 Score: 0.1732\n",
      "Epoch 6:\n",
      "Train Mean Squared Error: 0.0216\tMean Absolute Error: 0.1157\t Median Absolute Error: 0.0952\t R2 Score: 0.3228\n",
      "Valid Mean Squared Error: 0.0243\tMean Absolute Error: 0.1224\t Median Absolute Error: 0.1006\t R2 Score: 0.2382 *\n",
      "Epoch 7:\n",
      "Train Mean Squared Error: 0.0212\tMean Absolute Error: 0.1156\t Median Absolute Error: 0.0970\t R2 Score: 0.3360\n",
      "Valid Mean Squared Error: 0.0240\tMean Absolute Error: 0.1228\t Median Absolute Error: 0.1024\t R2 Score: 0.2461 *\n",
      "Epoch 8:\n",
      "Train Mean Squared Error: 0.0210\tMean Absolute Error: 0.1148\t Median Absolute Error: 0.0954\t R2 Score: 0.3406\n",
      "Valid Mean Squared Error: 0.0240\tMean Absolute Error: 0.1221\t Median Absolute Error: 0.1010\t R2 Score: 0.2481 *\n",
      "Epoch 9:\n",
      "Train Mean Squared Error: 0.0206\tMean Absolute Error: 0.1128\t Median Absolute Error: 0.0925\t R2 Score: 0.3541\n",
      "Valid Mean Squared Error: 0.0237\tMean Absolute Error: 0.1208\t Median Absolute Error: 0.0987\t R2 Score: 0.2570 *\n",
      "Epoch 10:\n",
      "Train Mean Squared Error: 0.0202\tMean Absolute Error: 0.1131\t Median Absolute Error: 0.0952\t R2 Score: 0.3660\n",
      "Valid Mean Squared Error: 0.0235\tMean Absolute Error: 0.1216\t Median Absolute Error: 0.1016\t R2 Score: 0.2628 *\n",
      "Epoch 11:\n",
      "Train Mean Squared Error: 0.0199\tMean Absolute Error: 0.1116\t Median Absolute Error: 0.0928\t R2 Score: 0.3765\n",
      "Valid Mean Squared Error: 0.0234\tMean Absolute Error: 0.1210\t Median Absolute Error: 0.1002\t R2 Score: 0.2647 *\n",
      "Epoch 12:\n",
      "Train Mean Squared Error: 0.0197\tMean Absolute Error: 0.1112\t Median Absolute Error: 0.0929\t R2 Score: 0.3827\n",
      "Valid Mean Squared Error: 0.0233\tMean Absolute Error: 0.1207\t Median Absolute Error: 0.1004\t R2 Score: 0.2683 *\n",
      "Epoch 13:\n",
      "Train Mean Squared Error: 0.0198\tMean Absolute Error: 0.1116\t Median Absolute Error: 0.0933\t R2 Score: 0.3799\n",
      "Valid Mean Squared Error: 0.0233\tMean Absolute Error: 0.1207\t Median Absolute Error: 0.1004\t R2 Score: 0.2689 *\n",
      "Epoch 14:\n",
      "Train Mean Squared Error: 0.0195\tMean Absolute Error: 0.1112\t Median Absolute Error: 0.0939\t R2 Score: 0.3890\n",
      "Valid Mean Squared Error: 0.0233\tMean Absolute Error: 0.1212\t Median Absolute Error: 0.1016\t R2 Score: 0.2674\n",
      "Epoch 15:\n",
      "Train Mean Squared Error: 0.0195\tMean Absolute Error: 0.1108\t Median Absolute Error: 0.0929\t R2 Score: 0.3898\n",
      "Valid Mean Squared Error: 0.0232\tMean Absolute Error: 0.1207\t Median Absolute Error: 0.1007\t R2 Score: 0.2707 *\n",
      "Epoch 16:\n",
      "Train Mean Squared Error: 0.0193\tMean Absolute Error: 0.1106\t Median Absolute Error: 0.0933\t R2 Score: 0.3959\n",
      "Valid Mean Squared Error: 0.0233\tMean Absolute Error: 0.1213\t Median Absolute Error: 0.1017\t R2 Score: 0.2675\n",
      "Epoch 17:\n",
      "Train Mean Squared Error: 0.0189\tMean Absolute Error: 0.1092\t Median Absolute Error: 0.0920\t R2 Score: 0.4087\n",
      "Valid Mean Squared Error: 0.0232\tMean Absolute Error: 0.1207\t Median Absolute Error: 0.1005\t R2 Score: 0.2731 *\n",
      "Epoch 18:\n",
      "Train Mean Squared Error: 0.0190\tMean Absolute Error: 0.1094\t Median Absolute Error: 0.0919\t R2 Score: 0.4050\n",
      "Valid Mean Squared Error: 0.0232\tMean Absolute Error: 0.1207\t Median Absolute Error: 0.1008\t R2 Score: 0.2708\n",
      "Epoch 19:\n",
      "Train Mean Squared Error: 0.0188\tMean Absolute Error: 0.1090\t Median Absolute Error: 0.0914\t R2 Score: 0.4101\n",
      "Valid Mean Squared Error: 0.0231\tMean Absolute Error: 0.1204\t Median Absolute Error: 0.1002\t R2 Score: 0.2738 *\n",
      "Epoch 20:\n",
      "Train Mean Squared Error: 0.0184\tMean Absolute Error: 0.1074\t Median Absolute Error: 0.0896\t R2 Score: 0.4226\n",
      "Valid Mean Squared Error: 0.0232\tMean Absolute Error: 0.1204\t Median Absolute Error: 0.0997\t R2 Score: 0.2702\n",
      "Epoch 21:\n",
      "Train Mean Squared Error: 0.0184\tMean Absolute Error: 0.1075\t Median Absolute Error: 0.0898\t R2 Score: 0.4248\n",
      "Valid Mean Squared Error: 0.0232\tMean Absolute Error: 0.1204\t Median Absolute Error: 0.1003\t R2 Score: 0.2732\n",
      "Epoch 22:\n",
      "Train Mean Squared Error: 0.0184\tMean Absolute Error: 0.1077\t Median Absolute Error: 0.0904\t R2 Score: 0.4239\n",
      "Valid Mean Squared Error: 0.0232\tMean Absolute Error: 0.1205\t Median Absolute Error: 0.1003\t R2 Score: 0.2730\n",
      "Epoch 23:\n",
      "Train Mean Squared Error: 0.0183\tMean Absolute Error: 0.1072\t Median Absolute Error: 0.0894\t R2 Score: 0.4258\n",
      "Valid Mean Squared Error: 0.0232\tMean Absolute Error: 0.1202\t Median Absolute Error: 0.0995\t R2 Score: 0.2723\n",
      "Epoch 24:\n",
      "Train Mean Squared Error: 0.0183\tMean Absolute Error: 0.1075\t Median Absolute Error: 0.0903\t R2 Score: 0.4272\n",
      "Valid Mean Squared Error: 0.0233\tMean Absolute Error: 0.1207\t Median Absolute Error: 0.1008\t R2 Score: 0.2695\n",
      "Epoch 25:\n",
      "Train Mean Squared Error: 0.0180\tMean Absolute Error: 0.1057\t Median Absolute Error: 0.0874\t R2 Score: 0.4375\n",
      "Valid Mean Squared Error: 0.0232\tMean Absolute Error: 0.1200\t Median Absolute Error: 0.0989\t R2 Score: 0.2717\n",
      "Finished training after 25 epochs.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"str\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c79f374a0414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdnns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdnns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_architecture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequenceDNN_Regression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.arch.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate tuple (not \"str\") to tuple"
     ]
    }
   ],
   "source": [
    "for task in range(len(tasks)):\n",
    "    dnns[task].train(X_train, y_train[:, task].reshape(-1,1), (X_valid, y_valid[:, task].reshape(-1,1)))\n",
    "    dnns[task].plot_architecture(tasks[task] + '.png')\n",
    "    models.SequenceDNN_Regression.save(dnns[task], tasks[task] + '.arch.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fn = tasks[task][0] + '_' + tasks[task][1]\n",
    "dnns[task].plot_architecture('%s.png' % fn)\n",
    "models.SequenceDNN_Regression.save(dnns[task], '%s.arch.json' % fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "def print_perf(dnns, metric):\n",
    "    for model in dnns:\n",
    "        train_losses, valid_losses = [np.array([epoch_metrics[metric] for epoch_metrics in metrics])\n",
    "                                      for metrics in (model.train_metrics, model.valid_metrics)]\n",
    "\n",
    "        # Pretty sure early stopping works by taking the mean of losses, might want to double check\n",
    "        # mean_train = train with one task?\n",
    "        mean_train_losses = train_losses.mean(axis=1)\n",
    "        mean_valid_losses = valid_losses.mean(axis=1)\n",
    "        min_loss_indx = min(enumerate(mean_valid_losses), key=lambda x: x[1])[0]\n",
    "\n",
    "        gs = gridspec.GridSpec(3, 2)\n",
    "        f = plt.figure(figsize=(15,10))\n",
    "\n",
    "        for i in range(train_losses.shape[1]):\n",
    "            y_max = max(max(train_losses[:,i]), max(valid_losses[:,i])) * 1.1\n",
    "\n",
    "            ax = f.add_subplot(gs[i])\n",
    "\n",
    "            ax.plot(range(len(train_losses[:,i])), train_losses[:,i], label='Training',lw=2)\n",
    "            ax.plot(range(len(train_losses[:,i])), valid_losses[:,i], label='Validation', lw=2)\n",
    "\n",
    "            ax.plot([min_loss_indx, min_loss_indx], [0, y_max], 'k--', label='Early Stop')\n",
    "            if i == 0:\n",
    "                ax.legend(loc=\"best\")\n",
    "                ax.set_ylabel(metric)\n",
    "            ax.set_ylim((0,y_max))\n",
    "            ax.set_title(\"Task {}\".format(i))\n",
    "\n",
    "        y_max = max(max(mean_train_losses), max(mean_valid_losses)) * 1.1\n",
    "\n",
    "        ax = f.add_subplot(gs[train_losses.shape[1]])\n",
    "        ax.plot(range(len(mean_train_losses)), mean_train_losses, label='Training',lw=2)\n",
    "        ax.plot(range(len(mean_valid_losses)), mean_valid_losses, label='Validation', lw=2)\n",
    "\n",
    "        ax.plot([min_loss_indx, min_loss_indx], [0, y_max], 'k--', label='Early Stop')\n",
    "        ax.set_ylim((0,y_max))\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_title(\"Mean losses\")\n",
    "\n",
    "        plt.savefig(\"losses.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric = \"Mean Squared Error\"\n",
    "print_perf(model, metric)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:regression_dragonn_new]",
   "language": "python",
   "name": "conda-env-regression_dragonn_new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
