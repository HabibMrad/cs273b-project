{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from glob import glob\n",
    "from tensorflow.python.ops import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import h5py\n",
    "\n",
    "def get_total_num_examples(hdf5_file_list):\n",
    "    '''\n",
    "    Quickly extracts total examples represented in an hdf5 file list. Can \n",
    "    be used to calculate total steps to take (when 1 step represents going \n",
    "    through a batch of examples)\n",
    "    '''\n",
    "    \n",
    "    num_examples = 0\n",
    "    for filename in hdf5_file_list:\n",
    "        with h5py.File(filename,'r') as hf:\n",
    "            num_examples += hf['features'].shape[0]\n",
    "\n",
    "    return num_examples\n",
    "\n",
    "\n",
    "def get_fan_in(tensor, type='NHWC'):\n",
    "    '''\n",
    "    Get the fan in (number of in channels)\n",
    "    '''\n",
    "\n",
    "    return int(tensor.get_shape()[-1])\n",
    "\n",
    "def maxnorm(norm_val=7):\n",
    "    '''\n",
    "    Torch7 style maxnorm. After gradients are applied in a training step,\n",
    "    the weights of each layer are clipped. This maxnorm applies to \n",
    "    convolutional layers as well as fully connected layers and does \n",
    "    not apply to biases.\n",
    "    '''\n",
    "\n",
    "    weights = [v for v in tf.all_variables()\n",
    "               if ('weights' in v.name)]\n",
    "\n",
    "    for weight in weights:\n",
    "        op_name = '{}/maxnorm'.format(weight.name.split('/weights')[0])\n",
    "        maxnorm_update = weight.assign(\n",
    "            tf.clip_by_norm(weight, norm_val, axes=[0], name=op_name))\n",
    "        tf.add_to_collection(tf.GraphKeys.UPDATE_OPS,\n",
    "                             maxnorm_update)\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.ops import random_ops\n",
    "\n",
    "def torch_initializer(stdv, dtype=dtypes.float32):\n",
    "    '''\n",
    "    Torch7 style initializer. Can be used for weights and biases.\n",
    "    '''\n",
    "    def _initializer(shape, dtype=dtype, partition_info=None, seed=1337):\n",
    "        return random_ops.random_uniform(shape, -stdv, stdv, dtype, seed=seed)\n",
    "    return _initializer\n",
    "\n",
    "\n",
    "def torch_conv_initializer(filter_shape, fan_in, dtype=dtypes.float32):\n",
    "    '''\n",
    "    Wraps the calculation of the standard dev for convolutional layers:\n",
    "    stdv = 1 / sqrt( filter_width * filter_height * fan_in )\n",
    "    '''\n",
    "    stdv = 1. / math.sqrt(filter_shape[0] * filter_shape[1] * fan_in)\n",
    "    return torch_initializer(stdv)\n",
    "\n",
    "\n",
    "def torch_fullyconnected_initializer(fan_in, dtype=dtypes.float32):\n",
    "    '''\n",
    "    Wraps the calculation of the standard dev for fully connected layers:\n",
    "    stdv = 1 / sqrt( fan_in )\n",
    "    '''\n",
    "    stdv = 1. / math.sqrt(fan_in)\n",
    "    return torch_initializer(stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basset(features, labels, is_training=True):\n",
    "    '''\n",
    "    Basset - Kelley et al Genome Research 2016\n",
    "    '''\n",
    "\n",
    "    with slim.arg_scope([slim.conv2d], padding='VALID',\n",
    "                        activation_fn=None):\n",
    "\n",
    "        # Layer 1: conv layer to batch norm to relu to max pool.\n",
    "        conv1_filter_size = [1, 19]\n",
    "        net = slim.conv2d(\n",
    "            features, 300, conv1_filter_size,\n",
    "            weights_initializer=torch_conv_initializer(\n",
    "                conv1_filter_size, get_fan_in(features)),\n",
    "            biases_initializer=torch_conv_initializer(\n",
    "                conv1_filter_size, get_fan_in(features)),\n",
    "            scope='conv1/conv')\n",
    "        net = slim.batch_norm(net, center=True, scale=True,\n",
    "                              activation_fn=nn.relu, \n",
    "                              is_training=is_training,\n",
    "                              scope='conv1/batchnorm')\n",
    "        net = slim.max_pool2d(net, [1, 3], stride=[1, 3], \n",
    "            scope='conv1/maxpool')\n",
    "\n",
    "        # Layer 2: conv layer to batch norm to relu to max pool.\n",
    "        conv2_filter_size = [1, 11]\n",
    "        net = slim.conv2d(\n",
    "            net, 200, conv2_filter_size,\n",
    "            weights_initializer=torch_conv_initializer(\n",
    "                conv2_filter_size, get_fan_in(net)),\n",
    "            biases_initializer=torch_conv_initializer(\n",
    "                conv2_filter_size, get_fan_in(net)),\n",
    "            scope='conv2/conv')\n",
    "        net = slim.batch_norm(net, center=True, scale=True, \n",
    "                              activation_fn=nn.relu, \n",
    "                              is_training=is_training,\n",
    "                              scope='conv2/batchnorm')\n",
    "        net = slim.max_pool2d(net, [1, 4], stride=[1, 4], \n",
    "            scope='conv2/maxpool')\n",
    "\n",
    "    net = slim.flatten(net, scope='flatten')\n",
    "\n",
    "    with slim.arg_scope([slim.fully_connected], activation_fn=None):\n",
    "\n",
    "        # Layer 4: fully connected layer to relu to dropout\n",
    "        net = slim.fully_connected(\n",
    "            net, 1000, \n",
    "            weights_initializer=torch_fullyconnected_initializer(\n",
    "                get_fan_in(net)),\n",
    "            biases_initializer=torch_fullyconnected_initializer(\n",
    "                get_fan_in(net)),\n",
    "            scope='fullyconnected1/fullyconnected')\n",
    "        net = slim.batch_norm(net, center=True, scale=True,\n",
    "                              activation_fn=nn.relu, \n",
    "                              is_training=is_training,\n",
    "                              scope='fullyconnected1/batchnorm')\n",
    "        net = slim.dropout(net, keep_prob=0.7, is_training=is_training,\n",
    "            scope='fullyconnected1/dropout')\n",
    "\n",
    "        # Layer 5: fully connected layer to relu to dropout\n",
    "        net = slim.fully_connected(\n",
    "            net, 1000, \n",
    "            weights_initializer=torch_fullyconnected_initializer(\n",
    "                get_fan_in(net)),\n",
    "            biases_initializer=torch_fullyconnected_initializer(\n",
    "                get_fan_in(net)),\n",
    "            scope='fullyconnected2/fullyconnected')\n",
    "        net = slim.batch_norm(net, center=True, scale=True,\n",
    "                              activation_fn=nn.relu, \n",
    "                              is_training=is_training,\n",
    "                              scope='fullyconnected2/batchnorm')\n",
    "        net = slim.dropout(net, keep_prob=0.7, is_training=is_training,\n",
    "            scope='fullyconnected2/dropout')\n",
    "\n",
    "        # OUT\n",
    "        logits = slim.fully_connected(\n",
    "            net, int(labels.get_shape()[-1]), activation_fn=None,\n",
    "            weights_initializer=torch_fullyconnected_initializer(\n",
    "                get_fan_in(net)),\n",
    "            biases_initializer=torch_fullyconnected_initializer(\n",
    "                get_fan_in(net)),\n",
    "            scope='out')\n",
    "\n",
    "    # Torch7 style maxnorm\n",
    "    maxnorm(norm_val=7)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-98-36c17e947115>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-98-36c17e947115>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    features, labels, metadata =\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def train(X, y,\n",
    "          model_builder,\n",
    "          loss_fn,\n",
    "          optimizer_fn,\n",
    "          optimizer_params,\n",
    "          restore,\n",
    "          stopping_criterion,\n",
    "          batch_size,\n",
    "          OUT_DIR,\n",
    "          global_step_val):\n",
    "    '''\n",
    "    Wraps the routines needed for tf-slim\n",
    "    '''\n",
    "\n",
    "    with tf.Graph().as_default() as g:\n",
    "        \n",
    "        features, labels, metadata = \n",
    "\n",
    "        # model\n",
    "        predictions = model_builder(features, labels, is_training=True)\n",
    "\n",
    "        # loss\n",
    "        total_loss = loss_fn(predictions, labels)\n",
    "\n",
    "        # optimizer\n",
    "        optimizer = optimizer_fn(**optimizer_params)\n",
    "\n",
    "        # train op\n",
    "        train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "\n",
    "        # build metrics\n",
    "        summary_op = metrics_fn(total_loss, predictions, labels)\n",
    "\n",
    "        if restore:\n",
    "            checkpoint_path = tf.train.latest_checkpoint(OUT_DIR)\n",
    "            variables_to_restore = slim.get_model_variables()\n",
    "            variables_to_restore.append(slim.get_global_step()) \n",
    "            init_assign_op, init_feed_dict = slim.assign_from_checkpoint(\n",
    "                checkpoint_path,\n",
    "                variables_to_restore)\n",
    "            \n",
    "            # create init assignment function\n",
    "            def InitAssignFn(sess):\n",
    "                sess.run(init_assign_op, init_feed_dict)\n",
    "                \n",
    "            slim.learning.train(train_op,\n",
    "                                OUT_DIR,\n",
    "                                init_fn=InitAssignFn,\n",
    "                                number_of_steps=global_step_val,\n",
    "                                summary_op=summary_op,\n",
    "                                save_summaries_secs=20)\n",
    "\n",
    "        else:\n",
    "            slim.learning.train(train_op,\n",
    "                                OUT_DIR,\n",
    "                                number_of_steps=global_step_val,\n",
    "                                summary_op=summary_op,\n",
    "                                save_summaries_secs=20)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def setup_queue(features, labels, metadata, capacity=10000):\n",
    "    '''\n",
    "    Set up data queue as well as queue runner. The shapes of the\n",
    "    tensors are inferred from the inputs, so input shapes must be\n",
    "    set before this function is called.\n",
    "    '''\n",
    "\n",
    "    with tf.variable_scope('datalayer'):\n",
    "        queue = tf.FIFOQueue(capacity,\n",
    "                             [tf.float32, tf.float32, tf.string],\n",
    "                             shapes=[features.get_shape()[1:],\n",
    "                                     labels.get_shape()[1:],\n",
    "                                     metadata.get_shape()[1:]])\n",
    "        enqueue_op = queue.enqueue_many([features, labels, metadata])\n",
    "        queue_runner = tf.train.QueueRunner(\n",
    "            queue=queue,\n",
    "            enqueue_ops=[enqueue_op],\n",
    "            close_op=queue.close(),\n",
    "            cancel_op=queue.close(cancel_pending_enqueues=True))\n",
    "        tf.train.add_queue_runner(queue_runner, tf.GraphKeys.QUEUE_RUNNERS)\n",
    "\n",
    "    return queue\n",
    "\n",
    "\n",
    "def get_hdf5_list_reader_pyfunc(hdf5_files, batch_size):\n",
    "    '''\n",
    "    Takes in a list of hdf5 files and generates a tensorflow op that returns a \n",
    "    group of examples and labels when called in the graph. Be aware that this\n",
    "    setup uses global variables that must be initialized first to make this\n",
    "    work.\n",
    "    '''\n",
    "\n",
    "    # Get all file handles before starting learning.\n",
    "    h5py_handles = [ h5py.File(filename) for filename in hdf5_files ]\n",
    "\n",
    "    # Check shapes from the hdf5 file so that we can set the tensor shapes\n",
    "    feature_shape = h5py_handles[0]['features'].shape[1:]\n",
    "    label_shape = h5py_handles[0]['labels'].shape[1:]\n",
    "\n",
    "    def hdf5_reader_fn():\n",
    "        '''\n",
    "        Given batch start and stop, pulls those examples from hdf5 file\n",
    "        '''\n",
    "        global batch_start\n",
    "        global batch_end\n",
    "        global filename_index\n",
    "\n",
    "        # check if at end of file, and move on to the next file\n",
    "        if batch_end > h5py_handles[filename_index]['features'].shape[0]:\n",
    "            print hdf5_files[filename_index]\n",
    "            filename_index += 1\n",
    "            batch_start = 0\n",
    "            batch_end = batch_size\n",
    "\n",
    "        if filename_index >= len(h5py_handles):\n",
    "            filename_index = 0\n",
    "            batch_start = 0\n",
    "            batch_end = batch_size\n",
    "\n",
    "        current_handle = h5py_handles[filename_index]\n",
    "\n",
    "        features = current_handle['features'][batch_start:batch_end,:,:,:]\n",
    "        labels = current_handle['labels'][batch_start:batch_end,:]\n",
    "        metadata = current_handle['regions'][batch_start:batch_end].reshape(\n",
    "            (batch_size, 1))\n",
    "\n",
    "        batch_start += batch_size\n",
    "        batch_end += batch_size\n",
    "\n",
    "        return [features, labels, metadata]\n",
    "\n",
    "    [py_func_features, py_func_labels, py_func_metadata] = tf.py_func(\n",
    "        hdf5_reader_fn,\n",
    "        [],\n",
    "        [tf.float32, tf.float32, tf.string],\n",
    "        stateful=True)\n",
    "\n",
    "    # Set the shape so that we can infer sizes etc in later layers.\n",
    "    py_func_features.set_shape([batch_size,\n",
    "                                feature_shape[0],\n",
    "                                feature_shape[1],\n",
    "                                feature_shape[2]])\n",
    "    py_func_labels.set_shape([batch_size, label_shape[0]])\n",
    "    py_func_metadata.set_shape([batch_size, 1])\n",
    "    \n",
    "    return py_func_features, py_func_labels, py_func_metadata\n",
    "\n",
    "\n",
    "def load_data_from_filename_list(hdf5_files, batch_size):\n",
    "    '''\n",
    "    Put it all together\n",
    "    '''\n",
    "\n",
    "    global batch_start\n",
    "    global batch_end\n",
    "    global filename_index\n",
    "\n",
    "    batch_start = 0\n",
    "    batch_end = batch_size\n",
    "    filename_index = 0\n",
    "\n",
    "    [hdf5_features, hdf5_labels, hdf5_metadata] = get_hdf5_list_reader_pyfunc(hdf5_files,\n",
    "                                                                              batch_size)\n",
    "\n",
    "    queue = setup_queue(hdf5_features, hdf5_labels, hdf5_metadata)\n",
    "\n",
    "    [features, labels, metadata] = queue.dequeue_many(batch_size)\n",
    "\n",
    "    return features, labels, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(X_train, X_valid, y_train, y_valid):\n",
    "    # TODO fix input of info to make easier to run\n",
    "    OUT_DIR = './log'\n",
    "    batch_size = 128\n",
    "    epochs = 4\n",
    "\n",
    "    # This all needs to be cleaned up into some kind of init function...\n",
    "    num_train_examples = 1000 #y_train.get_shape()[0]\n",
    "    train_steps = num_train_examples / batch_size - 100\n",
    "    print train_steps\n",
    "    train_steps = 100 # for now. just to make it easier to test\n",
    "    print 'Num train examples: {}'.format(num_train_examples)\n",
    "\n",
    "    num_valid_examples = 50 #y_valid.get_shape()[0]\n",
    "    valid_steps = num_valid_examples / batch_size - 100\n",
    "    print 'Num valid examples: {}'.format(num_valid_examples)\n",
    "\n",
    "    # Should epoch level be where things are exposed here? or is this loop abstractable too?\n",
    "    for epoch in xrange(epochs):\n",
    "        print \"EPOCH:\", str(epoch)\n",
    "\n",
    "        restore = bool(epoch)\n",
    "\n",
    "        # Run training\n",
    "        train(X_train, y_train,\n",
    "              basset,\n",
    "              slim.losses.sigmoid_cross_entropy,\n",
    "            tf.train.RMSPropOptimizer,\n",
    "            {'learning_rate': 0.002, 'decay':0.98, 'momentum':0.0, 'epsilon':1e-8},\n",
    "            restore,\n",
    "            'Not yet implemented',\n",
    "            batch_size,\n",
    "            '{}/train'.format(OUT_DIR),\n",
    "            (epoch+1)*train_steps)\n",
    "\n",
    "        # Get last checkpoint\n",
    "        checkpoint_path = tf.train.latest_checkpoint('{}/train'.format(OUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-93\n",
      "Num train examples: 1000\n",
      "Num valid examples: 50\n",
      "EPOCH: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable conv1/conv/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/home/deeperswag/.conda/envs/tf-model/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 230, in variable\n    caching_device=caching_device)\n  File \"/home/deeperswag/.conda/envs/tf-model/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 171, in func_with_args\n    return func(*args, **current_args)\n  File \"/home/deeperswag/.conda/envs/tf-model/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 266, in model_variable\n    caching_device=caching_device, device=device)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-5a8effdda7f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-100-92a0161bc77e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(X_train, X_valid, y_train, y_valid)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;34m'{}/train'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             (epoch+1)*train_steps)\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Get last checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-8314d5a0aaf2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(features, labels, model_builder, loss_fn, optimizer_fn, optimizer_params, restore, stopping_criterion, batch_size, OUT_DIR, global_step_val)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-d981d773a0e2>\u001b[0m in \u001b[0;36mbasset\u001b[0;34m(features, labels, is_training)\u001b[0m\n\u001b[1;32m     15\u001b[0m             biases_initializer=torch_conv_initializer(\n\u001b[1;32m     16\u001b[0m                 conv1_filter_size, get_fan_in(features)),\n\u001b[0;32m---> 17\u001b[0;31m             scope='conv1/conv')\n\u001b[0m\u001b[1;32m     18\u001b[0m         net = slim.batch_norm(net, center=True, scale=True,\n\u001b[1;32m     19\u001b[0m                               \u001b[0mactivation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeperswag/.conda/envs/tf-model/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.pyc\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_key_op'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeperswag/.conda/envs/tf-model/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.pyc\u001b[0m in \u001b[0;36mconvolution2d\u001b[0;34m(inputs, num_outputs, kernel_size, stride, padding, rate, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope)\u001b[0m\n\u001b[1;32m    405\u001b[0m                                        \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m                                        \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_collections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                                        trainable=trainable)\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matrous_conv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeperswag/.conda/envs/tf-model/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.pyc\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_key_op'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeperswag/.conda/envs/tf-model/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/variables.pyc\u001b[0m in \u001b[0;36mmodel_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device)\u001b[0m\n\u001b[1;32m    264\u001b[0m                   \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                   \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                   caching_device=caching_device, device=device)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeperswag/.conda/envs/tf-model/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.pyc\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_key_op'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeperswag/.conda/envs/tf-model/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/variables.pyc\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device)\u001b[0m\n\u001b[1;32m    228\u001b[0m                                        \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                                        \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                                        caching_device=caching_device)\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeperswag/.conda/envs/tf-model/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m       custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeperswag/.conda/envs/tf-model/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    671\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m           custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/home/deeperswag/.conda/envs/tf-model/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    215\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m           validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/home/deeperswag/.conda/envs/tf-model/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[1;32m    200\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deeperswag/.conda/envs/tf-model/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[1;32m    492\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 494\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    495\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable conv1/conv/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/home/deeperswag/.conda/envs/tf-model/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 230, in variable\n    caching_device=caching_device)\n  File \"/home/deeperswag/.conda/envs/tf-model/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 171, in func_with_args\n    return func(*args, **current_args)\n  File \"/home/deeperswag/.conda/envs/tf-model/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 266, in model_variable\n    caching_device=caching_device, device=device)\n"
     ]
    }
   ],
   "source": [
    "X_train = tf.ones((1000, 4, 100, 1), dtype='float32')\n",
    "X_valid = np.ones((50, 4, 100, 1), dtype='float32')\n",
    "y_train = tf.ones((1000,1), dtype='float32')\n",
    "y_valid = np.ones((50,1), dtype='float32')\n",
    "main(X_train, X_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf-model]",
   "language": "python",
   "name": "conda-env-tf-model-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
