{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs two basic convolutional networks on the pilot dataset.\n",
    "\n",
    "### Things to do next\n",
    "- Figure out how to do regression (see below).\n",
    "- ~~Figure out how to do multitask learning (i.e. try to predict the different reps and different genes).~~\n",
    "- ~~Run on the scale up dataset.~~\n",
    "- Read up on the \"Interpreting a DragoNN model using filter visualization\" and \"Interpreting data with a DragoNN model\" in the Dragonn tutorial.\n",
    "\n",
    "### Installing Dragonn\n",
    "- Clone from https://github.com/kundajelab/dragonn\n",
    "- ```python setup.py```\n",
    "    - I needed to ```brew install geos```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5005)\n",
      "/usr/local/anaconda3/envs/regression_dragonn_new/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from dragonn import models\n",
    "from dragonn.plot import add_letters_to_axis\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "from warnings import warn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data.\n",
    "- Samples are sequences that will be one hot encoded\n",
    "- Try and predict the normalized values. **I couldn't figure out how to do regression with Dragonn, so I just rounded the values to 0 or 1 based on the median.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/regression_dragonn_new/lib/python2.7/site-packages/ipykernel/__main__.py:21: UserWarning: Replacing 'N' bases in seq with 'A' in seq NNNNNNNNNGATCCCGCGGCCGTGTTTTCCTGGTGGCCCGGCCGTGCCTGAGGTTTCTCCCCGAGCCGCCGCCTCTGCGGGCTCCCGGGTGCCCTTGCCCTCACGGTCCCCGGCCCTCGCCCGTCTGTGCCCTCTTCCCCGCCCG.\n",
      "/usr/local/anaconda3/envs/regression_dragonn_new/lib/python2.7/site-packages/ipykernel/__main__.py:21: UserWarning: Replacing 'N' bases in seq with 'A' in seq NNNNGATCCCGCGGCCGTGTTTTCCTGGTGGCCCGGCCGTGCCTGAGGTTTCTCCCCGAGCCGCCGCCTCTGCGGGCTCCCGGGTGCCCTTGCCCTCACGGTCCCCGGCCCTCGCCCGTCTGTGCCCTCTTCCCCGCCCGCCGCC.\n",
      "/usr/local/anaconda3/envs/regression_dragonn_new/lib/python2.7/site-packages/ipykernel/__main__.py:21: UserWarning: Replacing 'N' bases in seq with 'A' in seq NNNNNNNNNNNNNNNNNNGAATTCAATGGAGTGGACTGGAGTGCTGTGGGGTGGAGTGGAATGGAGTGTAGTTGAATGGAGTGGAATGGAATGCGATGGAATGGAGTGGAGTTGAGCAGAGTGAAGTGGAAAGGTGTAGAATGGA.\n",
      "/usr/local/anaconda3/envs/regression_dragonn_new/lib/python2.7/site-packages/ipykernel/__main__.py:21: UserWarning: Replacing 'N' bases in seq with 'A' in seq NNNNNNNNNNNNNGAATTCAATGGAGTGGACTGGAGTGCTGTGGGGTGGAGTGGAATGGAGTGTAGTTGAATGGAGTGGAATGGAATGCGATGGAATGGAGTGGAGTTGAGCAGAGTGAAGTGGAAAGGTGTAGAATGGAATGGA.\n",
      "/usr/local/anaconda3/envs/regression_dragonn_new/lib/python2.7/site-packages/ipykernel/__main__.py:21: UserWarning: Replacing 'N' bases in seq with 'A' in seq NNNNNNNNGAATTCAATGGAGTGGACTGGAGTGCTGTGGGGTGGAGTGGAATGGAGTGTAGTTGAATGGAGTGGAATGGAATGCGATGGAATGGAGTGGAGTTGAGCAGAGTGAAGTGGAAAGGTGTAGAATGGAATGGAATGGA.\n",
      "/usr/local/anaconda3/envs/regression_dragonn_new/lib/python2.7/site-packages/ipykernel/__main__.py:21: UserWarning: Replacing 'N' bases in seq with 'A' in seq NNNGAATTCAATGGAGTGGACTGGAGTGCTGTGGGGTGGAGTGGAATGGAGTGTAGTTGAATGGAGTGGAATGGAATGCGATGGAATGGAGTGGAGTTGAGCAGAGTGAAGTGGAAAGGTGTAGAATGGAATGGAATGGAGTGGA.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('H1hesc_1_0_0_chr20_30310735',\n",
      "  'GGGAGCCCAGAAGGCGACACAGGAATTGCGAAGCTCAGGAACCAGCCCCCTCGCTTGCTTCCTCCTCCATCGCCCGGATCGAGGGCGGCCGCTCCGCAGCCGCGGCCTCCTGCCACCCGGGAGCCCAGCCCCCTCTCTCTTGCAC'),\n",
      " ('H1hesc_1_0_1_chr20_30310735',\n",
      "  'CCCAGAAGGCGACACAGGAATTGCGAAGCTCAGGAACCAGCCCCCTCGCTTGCTTCCTCCTCCATCGCCCGGATCGAGGGCGGCCGCTCCGCAGCCGCGGCCTCCTGCCACCCGGGAGCCCAGCCCCCTCTCTCTTGCACGCCCC'),\n",
      " ('H1hesc_1_0_2_chr20_30310735',\n",
      "  'AAGGCGACACAGGAATTGCGAAGCTCAGGAACCAGCCCCCTCGCTTGCTTCCTCCTCCATCGCCCGGATCGAGGGCGGCCGCTCCGCAGCCGCGGCCTCCTGCCACCCGGGAGCCCAGCCCCCTCTCTCTTGCACGCCCCTTGGC'),\n",
      " ('H1hesc_1_0_3_chr20_30310735',\n",
      "  'GACACAGGAATTGCGAAGCTCAGGAACCAGCCCCCTCGCTTGCTTCCTCCTCCATCGCCCGGATCGAGGGCGGCCGCTCCGCAGCCGCGGCCTCCTGCCACCCGGGAGCCCAGCCCCCTCTCTCTTGCACGCCCCTTGGCTCTCC'),\n",
      " ('H1hesc_1_0_4_chr20_30310735',\n",
      "  'AGGAATTGCGAAGCTCAGGAACCAGCCCCCTCGCTTGCTTCCTCCTCCATCGCCCGGATCGAGGGCGGCCGCTCCGCAGCCGCGGCCTCCTGCCACCCGGGAGCCCAGCCCCCTCTCTCTTGCACGCCCCTTGGCTCTCCGCCTC')]\n",
      "487137 total sequences of length 145\n"
     ]
    }
   ],
   "source": [
    "key_to_seq = OrderedDict()\n",
    "\n",
    "with open(\"../data/Scaleup_counts_sequences/ScaleUpDesign1.sequences.txt\") as f:\n",
    "    for line in f:\n",
    "        key, seq = line.strip().split()\n",
    "        \n",
    "        # TODO: Figure out if this is an OK thing to do. 'N' basically means the \n",
    "        # sequencing software couldn't figure out what the base was...?\n",
    "        if \"N\" in seq:\n",
    "            warn(\"Replacing 'N' bases in seq with 'A' in seq {}.\".format(seq))\n",
    "            seq = seq.replace(\"N\", \"A\")\n",
    "        \n",
    "        assert key not in key_to_seq\n",
    "        key_to_seq[key] = seq\n",
    "        \n",
    "with open(\"../data/Scaleup_counts_sequences/ScaleUpDesign2.sequences.txt\") as f:\n",
    "    for line in f:\n",
    "        key, seq = line.strip().split()\n",
    "        \n",
    "        if \"N\" in seq:\n",
    "            warn(\"Replacing 'N' bases in seq with 'A' in seq {}.\".format(seq))\n",
    "            seq = seq.replace(\"N\", \"A\")\n",
    "        \n",
    "        assert key not in key_to_seq\n",
    "        key_to_seq[key] = seq\n",
    "        \n",
    "pprint(key_to_seq.items()[:5])\n",
    "\n",
    "print \"{} total sequences of length {}\".format(len(key_to_seq), len(key_to_seq.values()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from experiment ('HepG2', 'minP'):\n",
      "[('Huvec_15_5_12_chr9_134025315', -0.8987649255400356),\n",
      " ('K562_7_2_10_chr5_77817535', -2.6900887233895894),\n",
      " ('K562_8_39_18_chr11_5602495', -0.9276572495402888),\n",
      " ('Huvec_5_556_25_chr16_3727095', -0.6722025396114724),\n",
      " ('Hepg2_8_141_16_chr7_514575', 1.8181443314468417)]\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "cell_types =  [\"HepG2\", \"K562\"]\n",
    "promoters = [\"SV40P\", \"minP\"]\n",
    "design_names = [\"ScaleUpDesign1\", \"ScaleUpDesign2\"]\n",
    "\n",
    "for cell_type in cell_types:\n",
    "    for promoter in promoters:\n",
    "        experiment_key = (cell_type, promoter)\n",
    "        data[experiment_key] = {}\n",
    "\n",
    "        for design_name in design_names:\n",
    "\n",
    "            with open(\"../data/Scaleup_normalized/{}_{}_{}_mRNA_Rep1.normalized\".format(cell_type, design_name, promoter)) as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "\n",
    "                    key = parts[0]\n",
    "                    val = float(parts[1])\n",
    "                    if parts[2] == \"1\":\n",
    "                        data[experiment_key][key] = val\n",
    "\n",
    "            with open(\"../data/Scaleup_normalized/{}_{}_{}_mRNA_Rep2.normalized\".format(cell_type, design_name, promoter)) as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "\n",
    "                    key = parts[0]\n",
    "                    val = float(parts[1])\n",
    "                    if parts[2] == \"1\" and key in data[experiment_key]:\n",
    "                        data[experiment_key][key] = (val + data[experiment_key][key]) / 2.0\n",
    "            \n",
    "print \"Data from experiment {}:\".format(data.items()[0][0])\n",
    "pprint(data.items()[0][1].items()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ('HepG2', 'minP') has 332929 measurements.\n",
      "Experiment ('K562', 'minP') has 332929 measurements.\n",
      "Experiment ('HepG2', 'SV40P') has 296463 measurements.\n",
      "Experiment ('K562', 'SV40P') has 296463 measurements.\n"
     ]
    }
   ],
   "source": [
    "for key, val in data.items():\n",
    "    print \"Experiment {} has {} measurements.\".format(key, len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# One hot encode DNA sequences the standard way.\n",
    "bases = ['A', 'T', 'C', 'G']\n",
    "\n",
    "def one_hot_encode_seq(seq):\n",
    "    result = np.zeros((len(bases), len(seq)))\n",
    "    \n",
    "    for i, base in enumerate(seq):\n",
    "        result[bases.index(base), i] = 1\n",
    "\n",
    "    return result\n",
    "\n",
    "def seqs_to_encoded_matrix(seqs):\n",
    "    # Wrangle the data into a shape that Dragonn wants.\n",
    "    result = np.concatenate(\n",
    "        map(one_hot_encode_seq, seqs)\n",
    "    ).reshape(\n",
    "        len(seqs), 1, len(bases), len(seqs[0])\n",
    "    )\n",
    "    \n",
    "    # Check we actually did the encoding right.\n",
    "    for i in range(len(seqs)):\n",
    "        for j in range(len(seqs[0])):\n",
    "            assert sum(result[i, 0, :, j]) == 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We formulate this as a multi-task learning problem, where each cell type and promoter combo is a task, i.e. the tasks are \n",
    "- Some of the normalized scores are too noisy (as determined by the SHARPR software). For now, we only consider a sequence if it has a good measurement for all of the tasks\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277409 sequences have measurements for all tasks.\n"
     ]
    }
   ],
   "source": [
    "valid_keys = list(reduce(\n",
    "    lambda acc, d: acc.intersection(d.keys()), \n",
    "    data.values()[1:], \n",
    "    set(data.values()[0].keys())\n",
    "))\n",
    "\n",
    "print \"{} sequences have measurements for all tasks.\".format(len(valid_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = seqs_to_encoded_matrix([key_to_seq[key] for key in valid_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(277409, 1, 4, 145)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just round to the median, to make this a classification task for now.\n",
    "experiment_labels = []\n",
    "for experiment_key, key_to_normalized in data.items():\n",
    "    \n",
    "    filtered_normalized = [key_to_normalized[key] for key in valid_keys]\n",
    "    \n",
    "    median = np.median(filtered_normalized)\n",
    "    experiment_labels.append(\n",
    "        np.array(map(lambda val: val > median, filtered_normalized)).reshape(-1, 1)\n",
    "    )\n",
    "\n",
    "y = np.hstack(experiment_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(277409, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 1.,  1.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  1.,  1.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "         [ 1.,  0.,  0., ...,  0.,  1.,  1.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  0.,  1.,  0.],\n",
       "         [ 1.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "         [ 0.,  0.,  0., ...,  1.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 1.,  1.,  1., ...,  1.,  0.,  1.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 1.,  0.,  0., ...,  1.,  1.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  0.,  0.,  1.]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True, False,  True],\n",
       "       [False, False, False,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True, False, False, False],\n",
       "       [ True,  True,  True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a very small model. Train and plot the train and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = models.SequenceDNN_Regression(\n",
    "    seq_length=X_train.shape[3],\n",
    "    num_filters=[100, 100, 100],\n",
    "    conv_width=[15, 15, 15],\n",
    "    pool_width=40,\n",
    "    num_tasks=y_train.shape[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model (* indicates new best result)...\n",
      "Epoch 1:\n",
      "Train Task 0: Mean Squared Error: 0.2472\tMean Absolute Error: 0.4946\t Median Absolute Error: 0.4936\t R2 Score: 0.0110\n",
      "Task 1: Mean Squared Error: 0.2477\tMean Absolute Error: 0.4955\t Median Absolute Error: 0.4959\t R2 Score: 0.0092\n",
      "Task 2: Mean Squared Error: 0.2478\tMean Absolute Error: 0.4953\t Median Absolute Error: 0.4956\t R2 Score: 0.0090\n",
      "Task 3: Mean Squared Error: 0.2471\tMean Absolute Error: 0.4948\t Median Absolute Error: 0.4952\t R2 Score: 0.0114\n",
      "Valid Task 0: Mean Squared Error: 0.2475\tMean Absolute Error: 0.4948\t Median Absolute Error: 0.4939\t R2 Score: 0.0099\n",
      "Task 1: Mean Squared Error: 0.2484\tMean Absolute Error: 0.4962\t Median Absolute Error: 0.4964\t R2 Score: 0.0064\n",
      "Task 2: Mean Squared Error: 0.2483\tMean Absolute Error: 0.4958\t Median Absolute Error: 0.4961\t R2 Score: 0.0067\n",
      "Task 3: Mean Squared Error: 0.2476\tMean Absolute Error: 0.4953\t Median Absolute Error: 0.4956\t R2 Score: 0.0095 *\n",
      "Epoch 2:\n",
      "Train Task 0: Mean Squared Error: 0.2455\tMean Absolute Error: 0.4919\t Median Absolute Error: 0.4929\t R2 Score: 0.0179\n",
      "Task 1: Mean Squared Error: 0.2478\tMean Absolute Error: 0.4930\t Median Absolute Error: 0.4913\t R2 Score: 0.0090\n",
      "Task 2: Mean Squared Error: 0.2456\tMean Absolute Error: 0.4928\t Median Absolute Error: 0.4926\t R2 Score: 0.0176\n",
      "Task 3: Mean Squared Error: 0.2472\tMean Absolute Error: 0.4931\t Median Absolute Error: 0.4936\t R2 Score: 0.0110\n",
      "Valid Task 0: Mean Squared Error: 0.2462\tMean Absolute Error: 0.4926\t Median Absolute Error: 0.4936\t R2 Score: 0.0152\n",
      "Task 1: Mean Squared Error: 0.2486\tMean Absolute Error: 0.4938\t Median Absolute Error: 0.4921\t R2 Score: 0.0057\n",
      "Task 2: Mean Squared Error: 0.2464\tMean Absolute Error: 0.4936\t Median Absolute Error: 0.4934\t R2 Score: 0.0146\n",
      "Task 3: Mean Squared Error: 0.2478\tMean Absolute Error: 0.4936\t Median Absolute Error: 0.4941\t R2 Score: 0.0089 *\n",
      "Epoch 3:\n",
      "Train Task 0: Mean Squared Error: 0.2440\tMean Absolute Error: 0.4864\t Median Absolute Error: 0.4892\t R2 Score: 0.0242\n",
      "Task 1: Mean Squared Error: 0.2428\tMean Absolute Error: 0.4877\t Median Absolute Error: 0.4894\t R2 Score: 0.0287\n",
      "Task 2: Mean Squared Error: 0.2425\tMean Absolute Error: 0.4869\t Median Absolute Error: 0.4897\t R2 Score: 0.0302\n",
      "Task 3: Mean Squared Error: 0.2448\tMean Absolute Error: 0.4900\t Median Absolute Error: 0.4920\t R2 Score: 0.0207\n",
      "Valid Task 0: Mean Squared Error: 0.2451\tMean Absolute Error: 0.4876\t Median Absolute Error: 0.4905\t R2 Score: 0.0196\n",
      "Task 1: Mean Squared Error: 0.2448\tMean Absolute Error: 0.4897\t Median Absolute Error: 0.4915\t R2 Score: 0.0209\n",
      "Task 2: Mean Squared Error: 0.2447\tMean Absolute Error: 0.4891\t Median Absolute Error: 0.4918\t R2 Score: 0.0214\n",
      "Task 3: Mean Squared Error: 0.2465\tMean Absolute Error: 0.4918\t Median Absolute Error: 0.4931\t R2 Score: 0.0138 *\n",
      "Epoch 4:\n",
      "Train Task 0: Mean Squared Error: 0.2380\tMean Absolute Error: 0.4806\t Median Absolute Error: 0.4817\t R2 Score: 0.0480\n",
      "Task 1: Mean Squared Error: 0.2392\tMean Absolute Error: 0.4824\t Median Absolute Error: 0.4828\t R2 Score: 0.0432\n",
      "Task 2: Mean Squared Error: 0.2376\tMean Absolute Error: 0.4803\t Median Absolute Error: 0.4817\t R2 Score: 0.0495\n",
      "Task 3: Mean Squared Error: 0.2419\tMean Absolute Error: 0.4857\t Median Absolute Error: 0.4859\t R2 Score: 0.0325\n",
      "Valid Task 0: Mean Squared Error: 0.2411\tMean Absolute Error: 0.4838\t Median Absolute Error: 0.4846\t R2 Score: 0.0355\n",
      "Task 1: Mean Squared Error: 0.2424\tMean Absolute Error: 0.4856\t Median Absolute Error: 0.4861\t R2 Score: 0.0305\n",
      "Task 2: Mean Squared Error: 0.2417\tMean Absolute Error: 0.4844\t Median Absolute Error: 0.4857\t R2 Score: 0.0331\n",
      "Task 3: Mean Squared Error: 0.2449\tMean Absolute Error: 0.4888\t Median Absolute Error: 0.4889\t R2 Score: 0.0206 *\n",
      "Epoch 5:\n",
      "Train Task 0: Mean Squared Error: 0.2328\tMean Absolute Error: 0.4723\t Median Absolute Error: 0.4750\t R2 Score: 0.0687\n",
      "Task 1: Mean Squared Error: 0.2350\tMean Absolute Error: 0.4758\t Median Absolute Error: 0.4789\t R2 Score: 0.0598\n",
      "Task 2: Mean Squared Error: 0.2321\tMean Absolute Error: 0.4703\t Median Absolute Error: 0.4721\t R2 Score: 0.0716\n",
      "Task 3: Mean Squared Error: 0.2389\tMean Absolute Error: 0.4818\t Median Absolute Error: 0.4826\t R2 Score: 0.0443\n",
      "Valid Task 0: Mean Squared Error: 0.2388\tMean Absolute Error: 0.4785\t Median Absolute Error: 0.4816\t R2 Score: 0.0447\n",
      "Task 1: Mean Squared Error: 0.2409\tMean Absolute Error: 0.4818\t Median Absolute Error: 0.4848\t R2 Score: 0.0364\n",
      "Task 2: Mean Squared Error: 0.2394\tMean Absolute Error: 0.4778\t Median Absolute Error: 0.4802\t R2 Score: 0.0426\n",
      "Task 3: Mean Squared Error: 0.2442\tMean Absolute Error: 0.4872\t Median Absolute Error: 0.4879\t R2 Score: 0.0232 *\n",
      "Epoch 6:\n",
      "Train Task 0: Mean Squared Error: 0.2292\tMean Absolute Error: 0.4641\t Median Absolute Error: 0.4613\t R2 Score: 0.0832\n",
      "Task 1: Mean Squared Error: 0.2321\tMean Absolute Error: 0.4699\t Median Absolute Error: 0.4681\t R2 Score: 0.0717\n",
      "Task 2: Mean Squared Error: 0.2272\tMean Absolute Error: 0.4630\t Median Absolute Error: 0.4613\t R2 Score: 0.0914\n",
      "Task 3: Mean Squared Error: 0.2376\tMean Absolute Error: 0.4771\t Median Absolute Error: 0.4755\t R2 Score: 0.0497\n",
      "Valid Task 0: Mean Squared Error: 0.2388\tMean Absolute Error: 0.4741\t Median Absolute Error: 0.4715\t R2 Score: 0.0448\n",
      "Task 1: Mean Squared Error: 0.2413\tMean Absolute Error: 0.4793\t Median Absolute Error: 0.4782\t R2 Score: 0.0350\n",
      "Task 2: Mean Squared Error: 0.2382\tMean Absolute Error: 0.4745\t Median Absolute Error: 0.4732\t R2 Score: 0.0473\n",
      "Task 3: Mean Squared Error: 0.2456\tMean Absolute Error: 0.4854\t Median Absolute Error: 0.4840\t R2 Score: 0.0175\n",
      "Epoch 7:\n",
      "Train Task 0: Mean Squared Error: 0.2209\tMean Absolute Error: 0.4534\t Median Absolute Error: 0.4529\t R2 Score: 0.1164\n",
      "Task 1: Mean Squared Error: 0.2255\tMean Absolute Error: 0.4612\t Median Absolute Error: 0.4637\t R2 Score: 0.0981\n",
      "Task 2: Mean Squared Error: 0.2206\tMean Absolute Error: 0.4507\t Median Absolute Error: 0.4464\t R2 Score: 0.1178\n",
      "Task 3: Mean Squared Error: 0.2319\tMean Absolute Error: 0.4702\t Median Absolute Error: 0.4695\t R2 Score: 0.0722\n",
      "Valid Task 0: Mean Squared Error: 0.2354\tMean Absolute Error: 0.4688\t Median Absolute Error: 0.4694\t R2 Score: 0.0583\n",
      "Task 1: Mean Squared Error: 0.2392\tMean Absolute Error: 0.4755\t Median Absolute Error: 0.4792\t R2 Score: 0.0434\n",
      "Task 2: Mean Squared Error: 0.2370\tMean Absolute Error: 0.4682\t Median Absolute Error: 0.4649\t R2 Score: 0.0520\n",
      "Task 3: Mean Squared Error: 0.2437\tMean Absolute Error: 0.4824\t Median Absolute Error: 0.4823\t R2 Score: 0.0251 *\n",
      "Epoch 8:\n",
      "Train Task 0: Mean Squared Error: 0.2159\tMean Absolute Error: 0.4419\t Median Absolute Error: 0.4444\t R2 Score: 0.1365\n",
      "Task 1: Mean Squared Error: 0.2217\tMean Absolute Error: 0.4510\t Median Absolute Error: 0.4493\t R2 Score: 0.1131\n",
      "Task 2: Mean Squared Error: 0.2132\tMean Absolute Error: 0.4374\t Median Absolute Error: 0.4385\t R2 Score: 0.1470\n",
      "Task 3: Mean Squared Error: 0.2276\tMean Absolute Error: 0.4608\t Median Absolute Error: 0.4595\t R2 Score: 0.0896\n",
      "Valid Task 0: Mean Squared Error: 0.2362\tMean Absolute Error: 0.4633\t Median Absolute Error: 0.4679\t R2 Score: 0.0554\n",
      "Task 1: Mean Squared Error: 0.2400\tMean Absolute Error: 0.4701\t Median Absolute Error: 0.4703\t R2 Score: 0.0401\n",
      "Task 2: Mean Squared Error: 0.2354\tMean Absolute Error: 0.4608\t Median Absolute Error: 0.4649\t R2 Score: 0.0583\n",
      "Task 3: Mean Squared Error: 0.2448\tMean Absolute Error: 0.4787\t Median Absolute Error: 0.4791\t R2 Score: 0.0209\n",
      "Epoch 9:\n",
      "Train Task 0: Mean Squared Error: 0.2137\tMean Absolute Error: 0.4352\t Median Absolute Error: 0.4382\t R2 Score: 0.1451\n",
      "Task 1: Mean Squared Error: 0.2195\tMean Absolute Error: 0.4457\t Median Absolute Error: 0.4511\t R2 Score: 0.1220\n",
      "Task 2: Mean Squared Error: 0.2100\tMean Absolute Error: 0.4291\t Median Absolute Error: 0.4312\t R2 Score: 0.1600\n",
      "Task 3: Mean Squared Error: 0.2234\tMean Absolute Error: 0.4557\t Median Absolute Error: 0.4562\t R2 Score: 0.1065\n",
      "Valid Task 0: Mean Squared Error: 0.2389\tMean Absolute Error: 0.4617\t Median Absolute Error: 0.4678\t R2 Score: 0.0445\n",
      "Task 1: Mean Squared Error: 0.2427\tMean Absolute Error: 0.4698\t Median Absolute Error: 0.4774\t R2 Score: 0.0292\n",
      "Task 2: Mean Squared Error: 0.2380\tMean Absolute Error: 0.4587\t Median Absolute Error: 0.4642\t R2 Score: 0.0480\n",
      "Task 3: Mean Squared Error: 0.2454\tMean Absolute Error: 0.4787\t Median Absolute Error: 0.4812\t R2 Score: 0.0183\n",
      "Epoch 10:\n",
      "Train Task 0: Mean Squared Error: 0.2067\tMean Absolute Error: 0.4236\t Median Absolute Error: 0.4159\t R2 Score: 0.1731\n",
      "Task 1: Mean Squared Error: 0.2135\tMean Absolute Error: 0.4375\t Median Absolute Error: 0.4328\t R2 Score: 0.1462\n",
      "Task 2: Mean Squared Error: 0.2022\tMean Absolute Error: 0.4179\t Median Absolute Error: 0.4091\t R2 Score: 0.1911\n",
      "Task 3: Mean Squared Error: 0.2215\tMean Absolute Error: 0.4488\t Median Absolute Error: 0.4439\t R2 Score: 0.1140\n",
      "Valid Task 0: Mean Squared Error: 0.2372\tMean Absolute Error: 0.4559\t Median Absolute Error: 0.4535\t R2 Score: 0.0513\n",
      "Task 1: Mean Squared Error: 0.2419\tMean Absolute Error: 0.4673\t Median Absolute Error: 0.4653\t R2 Score: 0.0322\n",
      "Task 2: Mean Squared Error: 0.2361\tMean Absolute Error: 0.4539\t Median Absolute Error: 0.4499\t R2 Score: 0.0555\n",
      "Task 3: Mean Squared Error: 0.2481\tMean Absolute Error: 0.4764\t Median Absolute Error: 0.4731\t R2 Score: 0.0077\n",
      "Epoch 11:\n",
      "Train Task 0: Mean Squared Error: 0.2025\tMean Absolute Error: 0.4162\t Median Absolute Error: 0.4078\t R2 Score: 0.1899\n",
      "Task 1: Mean Squared Error: 0.2090\tMean Absolute Error: 0.4295\t Median Absolute Error: 0.4277\t R2 Score: 0.1640\n",
      "Task 2: Mean Squared Error: 0.1971\tMean Absolute Error: 0.4087\t Median Absolute Error: 0.3998\t R2 Score: 0.2118\n",
      "Task 3: Mean Squared Error: 0.2159\tMean Absolute Error: 0.4413\t Median Absolute Error: 0.4394\t R2 Score: 0.1365\n",
      "Valid Task 0: Mean Squared Error: 0.2377\tMean Absolute Error: 0.4533\t Median Absolute Error: 0.4503\t R2 Score: 0.0494\n",
      "Task 1: Mean Squared Error: 0.2428\tMean Absolute Error: 0.4647\t Median Absolute Error: 0.4669\t R2 Score: 0.0289\n",
      "Task 2: Mean Squared Error: 0.2370\tMean Absolute Error: 0.4507\t Median Absolute Error: 0.4472\t R2 Score: 0.0519\n",
      "Task 3: Mean Squared Error: 0.2481\tMean Absolute Error: 0.4749\t Median Absolute Error: 0.4746\t R2 Score: 0.0075\n",
      "Epoch 12:\n",
      "Train Task 0: Mean Squared Error: 0.2047\tMean Absolute Error: 0.4124\t Median Absolute Error: 0.3944\t R2 Score: 0.1812\n",
      "Task 1: Mean Squared Error: 0.2201\tMean Absolute Error: 0.4267\t Median Absolute Error: 0.4015\t R2 Score: 0.1196\n",
      "Task 2: Mean Squared Error: 0.1959\tMean Absolute Error: 0.4024\t Median Absolute Error: 0.3853\t R2 Score: 0.2166\n",
      "Task 3: Mean Squared Error: 0.2212\tMean Absolute Error: 0.4390\t Median Absolute Error: 0.4273\t R2 Score: 0.1154\n",
      "Valid Task 0: Mean Squared Error: 0.2437\tMean Absolute Error: 0.4533\t Median Absolute Error: 0.4416\t R2 Score: 0.0251\n",
      "Task 1: Mean Squared Error: 0.2556\tMean Absolute Error: 0.4636\t Median Absolute Error: 0.4466\t R2 Score: -0.0222\n",
      "Task 2: Mean Squared Error: 0.2407\tMean Absolute Error: 0.4495\t Median Absolute Error: 0.4406\t R2 Score: 0.0373\n",
      "Task 3: Mean Squared Error: 0.2568\tMean Absolute Error: 0.4760\t Median Absolute Error: 0.4696\t R2 Score: -0.0274\n",
      "Epoch 13:\n",
      "Train Task 0: Mean Squared Error: 0.1987\tMean Absolute Error: 0.4044\t Median Absolute Error: 0.3970\t R2 Score: 0.2051\n",
      "Task 1: Mean Squared Error: 0.2101\tMean Absolute Error: 0.4184\t Median Absolute Error: 0.4167\t R2 Score: 0.1598\n",
      "Task 2: Mean Squared Error: 0.1911\tMean Absolute Error: 0.3941\t Median Absolute Error: 0.3824\t R2 Score: 0.2355\n",
      "Task 3: Mean Squared Error: 0.2175\tMean Absolute Error: 0.4301\t Median Absolute Error: 0.4252\t R2 Score: 0.1302\n",
      "Valid Task 0: Mean Squared Error: 0.2451\tMean Absolute Error: 0.4528\t Median Absolute Error: 0.4533\t R2 Score: 0.0197\n",
      "Task 1: Mean Squared Error: 0.2545\tMean Absolute Error: 0.4640\t Median Absolute Error: 0.4697\t R2 Score: -0.0178\n",
      "Task 2: Mean Squared Error: 0.2441\tMean Absolute Error: 0.4495\t Median Absolute Error: 0.4474\t R2 Score: 0.0234\n",
      "Task 3: Mean Squared Error: 0.2593\tMean Absolute Error: 0.4732\t Median Absolute Error: 0.4746\t R2 Score: -0.0372\n",
      "Finished training after 13 epochs.\n"
     ]
    }
   ],
   "source": [
    "model.train(X_train, y_train, (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fn = \"model\"\n",
    "models.SequenceDNN_Regression.save(model, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_loss(model):\n",
    "    train_losses, valid_losses = [np.array([epoch_metrics['Loss'] for epoch_metrics in metrics])\n",
    "                                  for metrics in (model.train_metrics, model.valid_metrics)]\n",
    "\n",
    "    # Pretty sure early stopping works by taking the mean of losses, might want to double check\n",
    "    train_losses = train_losses.mean(axis=1)\n",
    "    valid_losses = valid_losses.mean(axis=1)\n",
    "\n",
    "    f = plt.figure(figsize=(10, 4))\n",
    "    ax = f.add_subplot(1, 1, 1)\n",
    "    \n",
    "    ax.plot(range(len(train_losses)), train_losses, label='Training',lw=4)\n",
    "    ax.plot(range(len(train_losses)), valid_losses, label='Validation', lw=4)\n",
    "    \n",
    "    min_loss_indx = min(enumerate(valid_losses), key=lambda x: x[1])[0]\n",
    "    ax.plot([min_loss_indx, min_loss_indx], [0, 1.0], 'k--', label='Early Stop')\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_ylim((0.0,1.0))\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6261619861ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-31059fe62763>\u001b[0m in \u001b[0;36mprint_loss\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     train_losses, valid_losses = [np.array([epoch_metrics['Loss'] for epoch_metrics in metrics])\n\u001b[0;32m----> 3\u001b[0;31m                                   for metrics in (model.train_metrics, model.valid_metrics)]\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Pretty sure early stopping works by taking the mean of losses, might want to double check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/regression_dragonn_new/lib/python2.7/site-packages/dragonn-0.1.2-py2.7.egg/dragonn/metrics.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtask_results\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'Loss'"
     ]
    }
   ],
   "source": [
    "print_loss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_sequence_filters(dnn):\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    conv_filters = dnn.get_sequence_filters()\n",
    "    num_plots_per_axis = int(len(conv_filters)**0.5) + 1\n",
    "    for i, conv_filter in enumerate(conv_filters):\n",
    "        ax = fig.add_subplot(num_plots_per_axis, num_plots_per_axis, i+1)\n",
    "        add_letters_to_axis(ax, conv_filter.T)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(\"Filter %s\" % (str(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_sequence_filters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a larger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multi_filter_model = models.SequenceDNN(\n",
    "    seq_length=X_train.shape[3],\n",
    "    num_filters=[15, 15, 15],\n",
    "    conv_width=[15, 15, 15],\n",
    "    num_tasks=y_train.shape[1],\n",
    "    dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multi_filter_model.train(X_train, y_train, (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_loss(multi_filter_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:regression_dragonn_new]",
   "language": "python",
   "name": "conda-env-regression_dragonn_new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
